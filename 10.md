#RNN
##基本单层网络  
y = f(Wx + b)  

###经典RNN （N vs N）  
引入隐藏状态hidden state概念  
h1 = f(Ux1 + Wh0 + b)  
输出  
y = softmax(Vh1 + c)  
应用场景： 视频中每一帧的分类标签。  

###N vs 1
只在最后一个h上变换  
应用场景： 输入一段文字，判断它所属类别。  

###1 vs N
1 只在h1输入x  
2 每一个h输入x  
应用场景： 从类别生成音乐  

###N vs M
这种结构又叫做encoder-decoder模型，也可以称之为seq2seq模型。  
encoder，最后一个h4赋值c  
decoder，c当作初始状态h0输入  
应用场景：   
+ 机器翻译，源语言和目标语言的句子往往没有相同的长度。  
+ 文本摘要
+ 阅读理解，输入是文章和问题，输出是问题答案。  
+ 语音识别，输入语音，输出文字。  

###attention
问题起源： 要翻译的句子比较长时，一个c可能存不下那么多信息。  
attention机制通过在每个时间输入不同的c来解决这个问题。  
每个c自动选取与当前要输出的y最合适的上下文信息。  

#LSTM
起源： 对于RNN，很长时刻以前的输入，对现在的网络影响非常小，LSTM通过一些门，让网络记住那些非常重要的信息。
LSTM从外部看和RNN完全一样，上面所有结构对LSTM通用。

#GRU
效果堪比LSTM，但用到的参数更少  

#attention is all you need
seq2seq上的创新，抛弃RNN结构来做seq2seq  
RNN一个明显缺点是无法并行，速度慢，递归的天然缺陷。  
multi-head attention:  
多做几次同样的事情，参数不共享，然后把结构拼接。  

+ attention好处是一步到位捕捉全局的联系。  
+ RNN一步步递推
+ CNN通过层叠来扩大感受野

