### NER 的各种方法 ###
1. 基于规则  
	例子： “大学”作为机构名的结尾  

2. 特征模板 + 统计学习 
	统计机器学习方法将 NER 视作序列标注任务，利用大规模语料来学习出标注模型，从而对句子的各个位置进行标注。  

	HMM  
	CRF 

	特征模板通常是人工定义的一些二值特征函数，试图挖掘命名实体内部以及上下文的构成特点。    

	CRF的优点在于其为一个位置进行标注的过程中可以利用到此前已经标注的信息，利用Viterbi解码来得到最优序列。  

3. 基于神经网络的方法  
	将token从离散one-hot表示映射到低维空间中成为稠密的embedding  
	随后将句子的embedding序列输入到RNN中，用神经网络自动提取特征  
	Softmax来预测每个token的标签 
	不依赖特征工程   
	但网络变种多、对参数设置依赖大，模型可解释性差。
	此外，这种方法的一个缺点是对每个token打标签的过程中是独立的分类，不能直接利用上文已经预测的标签.

	***模型的第一层是 look-up 层，利用预训练或随机初始化的embedding矩阵将句子中的每个字 xixi 由one-hot向量映射为低维稠密的字向量***   
	在输入下一层之前，设置dropout以缓解过拟合。 
 
	***模型的第二层是双向LSTM层，自动提取句子特征。***   
	
	将一个句子的各个字的char embedding序列 (x1,x2,...,xn)(x1,x2,...,xn) 作为双向LSTM各个时间步的输入，再将正向LSTM输出的隐状态序列 (h1⟶,h2⟶,...,hn⟶)(h1⟶,h2⟶,...,hn⟶) 与反向LSTM的 (h1⟵,h2⟵,...,hn⟵)(h1⟵,h2⟵,...,hn⟵) 在各个位置输出的隐状态进行按位置拼接 ht=[ht⟶;ht⟵]∈Rmht=[ht⟶;ht⟵]∈Rm ，得到完整的隐状态序列(h1,h2,...,hn)∈Rn×m  
	

	在设置dropout后，接入一个线性层，将隐状态向量从 mm 维映射到 kk 维，kk 是标注集的标签数，从而得到自动提取的句子特征，记作矩阵 P=(p1,p2,...,pn)∈Rn×kP=(p1,p2,...,pn)∈Rn×k 。  
	可以把 pi∈Rkpi∈Rk 的每一维 pijpij 都视作将字 xixi 分类到第 jj 个标签的打分值，如果再对 PP 进行Softmax的话，就相当于对各个位置独立进行 kk 类分类。但是这样对各个位置进行标注时无法利用已经标注过的信息，所以接下来将接入一个CRF层来进行标注。  

	***模型的第三层是CRF层，进行句子级的序列标注。***  

	
